{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Academic Achievement Gap Analysis: \n",
    "## Data Cleaning, Wrangling, and Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "Upon graduating from university with a degree in International Relations, I joined an alternative teacher certification program called Teach for America (TFA). Why become an educator after writing a senior thesis on U.S. anti-narcotics policies in the 1980s? Because of TFA's focus on social justice. \n",
    "\n",
    "TFA's primary purpose is to close the \"achievement gap\" and end educational inequity - as it makes very clear in the \"About Us\" section of the TFA website: https://www.teachforamerica.org/about-us. What this means in practice is that TFA sends its teachers (\"corps members\") to teach in low-income schools across the country - generally where at least 80% of students at a school receive Title I funding. Such funds are provided to school districts to help support students living below the poverty line (as regulated by the \"Every Student Succeeds Act\").\n",
    "\n",
    "Like most corps members, I unquestioningly accepted TFA's premise regarding the relationship between poverty and school performance. But now I would like to explore the idea in greater depth, using data from the U.S. Department of Education's EDFacts Initiative, U.S. Census Bureau's Small Area Income and Poverty Estimates Program, and the U.S. Department of Education's Local Education Agency (School District) Universe Survey Data to answer the following question: Do students living in poorer communities perform worse on end-of-year reading and math tests? \n",
    "\n",
    "This notebook contains the data cleaning, wrangling and transformation that was required before this analysis could be completed. To see my final storytelling project, click <a href=\"https://nbviewer.jupyter.org/gist/TGasinski/2b2be58c17860f5868fbb084142a550c\">here</a>.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages and Data\n",
    "\n",
    "I will begin by importing all relevant packages and my first dataset - the district-level data on state math assessment results for the 2011-2012 school year. I can use the pd.set_option() and .head() functions to view a section of the data.\n",
    "\n",
    "__Data Source:__ https://catalog.data.gov/dataset/consolidated-state-performance-report-201112/resource/4da8f9de-d265-4e2f-b321-f180052df3f7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all needed packages\n",
    "import pandas as pd\n",
    "import csv as csv\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re \n",
    "import scipy\n",
    "\n",
    "\n",
    "#read in first set of data\n",
    "df_math_org= pd.read_csv(r'C:\\Users\\t_gas\\Desktop\\Data Portfolio\\Data Storytelling_Exploring the Academic Achievement Gap\\2011_2012_School_Data\\Test_Scores\\Math_District.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By reading the EdFacts data documentation, I can learn more about what each column header signifies. For example, the \"01\", \"02\", \"03\", etc. represent each grade level within that county.\n",
    "\n",
    "I know that I will want to compare data across counties but also across states. So before continuing, I will create a list of unique state names that I can use later in my analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create list of state names to use later in analysis\n",
    "state_names = []\n",
    " \n",
    "for x in df_math_org.iloc[:,0]:\n",
    "    if x in state_names:\n",
    "        continue\n",
    "    else:\n",
    "        state_names.append(x)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I know that I will only explore the proficiency levels of an entire county, rather than a specific grade within that county. From the data documentation, I know that \"00\" represents all grades, 1-8, within the county. So for the sake of my computer's RAM, I will create a smaller dataframe with only the relevant columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create smaller dataframe with only necessary columns\n",
    "df_math_all_grades=df_math_org[['STNAM', 'FIPST', 'LEAID', 'leanm11', 'ALL_MTH00pctprof_1112', 'MAM_MTH00pctprof_1112', 'MAS_MTH00pctprof_1112', 'MBL_MTH00pctprof_1112', 'MHI_MTH00pctprof_1112', 'MTR_MTH00pctprof_1112', 'MWH_MTH00pctprof_1112', 'F_MTH00pctprof_1112', 'M_MTH00pctprof_1112', 'CWD_MTH00pctprof_1112', 'ECD_MTH00pctprof_1112', 'LEP_MTH00pctprof_1112', 'HOM_MTH00pctprof_1112', 'MIG_MTH00pctprof_1112']].copy()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Because all of the values in the column \"ALL_MTH00pctprof_1112\" represent percentages of students, I should be able to convert them into numeric values. However, after examining the dataframe using .info(), it appears that all the values are saved as objects, not numbers (int64). This could mean that not all the saved values are numbers. In fact, as I examined the original dataframe, I already spotted data that will not be usable in its current state (e.g. the Math percent proficiency for Alabama Youth Services school was \"30-39\"). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Data\n",
    "\n",
    "By creating a set of values that cannot be converted, I can identify all the unique errors within the column. I will then create a for loop to fix the types of errors presented in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create list of unique errors within dataframe\n",
    "errors = set()\n",
    "\n",
    "for i in df_math_all_grades['MBL_MTH00pctprof_1112']:\n",
    "    try: \n",
    "        int(i)\n",
    "    except: \n",
    "        errors.add(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After executing print(errors), I have a clearer picture of what types of erroneous data exist in the dataframe and can decide how best to clean the data. \n",
    "\n",
    "When given two values (e.g. \"40-44\"), I will replace the two values with the values' mean. For all data including letters before a value, I will simply drop the letters. According to the documentation, \"PS\" means that the data has been removed to protect student privacy. In these instances, I will substitute \"PS\" with \"NaN.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create for loop to clean up each type of error found earlier in the dataframe\n",
    "for x,i in df_math_all_grades['MBL_MTH00pctprof_1112'].items():\n",
    "    i = (str(i))\n",
    "    if i in errors:\n",
    "        if i == \"nan\":\n",
    "            continue\n",
    "        elif \"-\" in i:\n",
    "            numbers = list(map(int, i.split(\"-\")))\n",
    "            replacement = np.mean(numbers)\n",
    "        elif \"PS\" in i: \n",
    "            replacement = np.nan\n",
    "        elif \"n/a\" in i: \n",
    "            replacement = np.nan\n",
    "        elif re.search(r\"[A-Za-z]\", i): \n",
    "            working_list = list(i)\n",
    "            numbers = []\n",
    "            for j in working_list:\n",
    "                if j.isnumeric():\n",
    "                    numbers.append(j)\n",
    "            replacement = int(\"\".join(numbers))\n",
    "        df_math_all_grades.iloc[x, 7] = replacement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can perform a quick check to ensure that no errors were overlooked, by printing the output of my simple loop using print(check)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test to make sure no more errors remain \n",
    "check = set()\n",
    "\n",
    "for i in df_math_all_grades['MBL_MTH00pctprof_1112']:\n",
    "    try: \n",
    "        int(i)\n",
    "    except: \n",
    "        check.add(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I will create a function in order to clean all the columns within my dataset. But before designing the function, I create two lists: a list of all column names within my dataset, and a list of column names that I do not want my function to clean (as the values represented are state and county names, along with county IDs, rather than test scores)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create lists of values to ignore in function\n",
    "column_names_math = list(df_math_all_grades)\n",
    "ignore = [\"STNAM\", \"FIPST\", \"LEAID\", \"leanm11\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use these lists in a function, which will clean up all the errors I identified earlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write function to clean columns within dataframe\n",
    "def cleaning_columns(df,column,order_number): \n",
    "    errors = set()\n",
    "    for i in column:\n",
    "        try:\n",
    "            int(i)\n",
    "        except:\n",
    "            errors.add(i)\n",
    "\n",
    "    for x,i in column.items():\n",
    "        i = (str(i))\n",
    "        if i in errors:\n",
    "            if i == \"nan\":\n",
    "                continue\n",
    "            elif \"-\" in i:\n",
    "                numbers = list(map(int, i.split(\"-\")))\n",
    "                replacement = np.mean(numbers)\n",
    "            elif \"PS\" in i:\n",
    "                replacement = np.nan\n",
    "            elif \"n/a\" in i:\n",
    "                replacement = np.nan\n",
    "            elif re.search(r\"[A-Za-z]\", i):\n",
    "                working_list = list(i)\n",
    "                numbers = []\n",
    "                for j in working_list:\n",
    "                    if j.isnumeric():\n",
    "                        numbers.append(j)\n",
    "                replacement = int(\"\".join(numbers))\n",
    "            \n",
    "            df.iloc[x, order_number] = replacement\n",
    "            \n",
    "    return(df)\n",
    "\n",
    "#run function\n",
    "for i in range(0,len(column_names_math)):\n",
    "    if column_names_math[i] in ignore:\n",
    "        continue\n",
    "    else:\n",
    "        name = column_names_math[i]\n",
    "        cleaning_columns(df_math_all_grades, df_math_all_grades[name], i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I create a second function that will allow me to quickly check that all of my data was cleaned correctly, by creating a function and then viewing the results using: print(check), as I did manually after the first column had been cleaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create function to ensure data was cleaned correctly\n",
    "def check_data(column):\n",
    "    check = set()\n",
    "    for i in column: \n",
    "        try: \n",
    "            int(i)\n",
    "        except: \n",
    "            check.add(i)\n",
    "\n",
    "#run function\n",
    "for i in range(0, len(column_names_math)): \n",
    "    if column_names_math[i] in ignore: \n",
    "        continue\n",
    "    else: \n",
    "        name = column_names_math[i]\n",
    "        check_data(df_math_all_grades[name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I can use the same functions to clean my second data set, which contains the district-level data on state reading assessment results for the 2011-2012 school year. \n",
    "\n",
    "__Data Source:__ https://catalog.data.gov/dataset/consolidated-state-performance-report-201112/resource/fd3382ec-8a1d-4372-930c-0e4f72db5037"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in second set of data and create unique list of column names \n",
    "df_reading_org= pd.read_csv(r'C:\\Users\\t_gas\\Desktop\\Data Portfolio\\Data Storytelling_Exploring the Academic Achievement Gap\\2011_2012_School_Data\\Test_Scores\\Reading_District.csv')\n",
    "df_reading_all_grades=df_reading_org[['STNAM', 'FIPST', 'LEAID', 'leanm11', 'ALL_RLA00pctprof_1112', 'MAM_RLA00pctprof_1112', 'MAS_RLA00pctprof_1112', 'MBL_RLA00pctprof_1112', 'MHI_RLA00pctprof_1112', 'MTR_RLA00pctprof_1112', 'MWH_RLA00pctprof_1112', 'F_RLA00pctprof_1112', 'M_RLA00pctprof_1112', 'CWD_RLA00pctprof_1112', 'ECD_RLA00pctprof_1112', 'LEP_RLA00pctprof_1112', 'HOM_RLA00pctprof_1112', 'MIG_RLA00pctprof_1112']].copy()  \n",
    "column_names_reading = list(df_reading_all_grades) \n",
    "ignore_reading = [\"STNAM\", \"FIPST\", \"LEAID\", \"leanm11\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run function to clean dataset\n",
    "for i in range(0,len(column_names_reading)):\n",
    "    if column_names_reading[i] in ignore_reading:\n",
    "        continue\n",
    "    else: \n",
    "        name = column_names_reading[i]\n",
    "        cleaning_columns(df_reading_all_grades, df_reading_all_grades[name], i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run function to ensure data was cleaned correctly\n",
    "for i in range(0, len(column_names_reading)): \n",
    "    if column_names_reading[i] in ignore: \n",
    "        continue\n",
    "    else: \n",
    "        name = column_names_reading[i]\n",
    "        check_data(df_reading_all_grades[name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Using .dtypes, I can see that almost all the rows in both dataframes have been stored as objects. This means that while the dataframes are clean, they are still not yet ready to be analyzed. In order to further explore the data, I need to convert the information into numeric values. \n",
    "\n",
    "To do so, I will build a function that ignores the first four rows (which contain text information, such as county name) and converts the rest of the column information (which contain percentages) into numeric values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create function to convert data into numeric values, then use .dtypes \n",
    "#to check that all values were converted correctly\n",
    "\n",
    "def numeric (df, column):\n",
    "    for column in df: \n",
    "        df[column] = df[column].apply(pd.to_numeric, errors = 'ignore')\n",
    "    return(df)\n",
    "\n",
    "for i in range(0, len(column_names_math)):\n",
    "    if column_names_math[i] in ignore: \n",
    "        continue\n",
    "    else: \n",
    "        name = column_names_math[i]\n",
    "        numeric(df_math_all_grades, df_math_all_grades[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run function and check value types using df.dtypes\n",
    "for i in range(0, len(column_names_reading)):\n",
    "    if column_names_reading[i] in ignore: \n",
    "        continue\n",
    "    else: \n",
    "        name = column_names_reading[i]\n",
    "        numeric(df_reading_all_grades, df_reading_all_grades[name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all the data has been converted into a numeric value, I can continue to explore the dataframes. I will begin by examining what percentage of my data contains NaN values, by creating a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create function to identify percent of data that contains nan values\n",
    "def percent_valid(column): \n",
    "    length = len(column)\n",
    "    nan = column.isnull().sum()\n",
    "    return ((length-nan)/length)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I were to run this function on both of my dataframes (e.g.: \"df_math_all_grades.apply(percent_valid, axis=0)\") I could see exactly what percentage NaN values existed in each column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Manipulate Dataframes\n",
    "\n",
    "I will create two smaller dataframes including only the columns with meaningful data (this will include the state name, county name, and total percent proficiency for both the Math and Reading state assessment results). Because these simplified dataframes contain a lot of the same information, I can then combine them into a single dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge dataframes\n",
    "df_math_only_totals = df_math_all_grades[['STNAM', 'LEAID', 'leanm11', 'ALL_MTH00pctprof_1112']].copy()\n",
    "df_reading_only_totals = df_reading_all_grades[['STNAM', 'LEAID', 'leanm11', 'ALL_RLA00pctprof_1112']].copy()\n",
    "df_all_states = pd.merge(df_math_only_totals, df_reading_only_totals, how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Next, I will import a dataset which shows the poverty rates per country in 2011 and 2012. \n",
    "\n",
    "To measure poverty, the Census Bureau assigns each family member one out of 48 possible \"poverty thresholds.\" These thresholds vary by the size of the family and the age of the family members, but the thresholds themselves do not vary geographically. The thresholds are also updated annually for inflation. The Census Bureau then calculates the total family income, by adding up the income of all family members that live together. \n",
    "\n",
    "If the total family income is less than the poverty threshold for that family, that family and every individual in it is considered to be inpoverty. If the income is equal to or greater than the poverty threshold, that family is not considered to be in poverty. For a complete explanation of how the Census Bureau measures poverty, see: https://www.census.gov/topics/income-poverty/poverty/guidance/poverty-measures.html\n",
    "\n",
    "__Data Sources:__ https://www.census.gov/data/datasets/2011/demo/saipe/2011-state-and-county.html and https://www.census.gov/data/datasets/2012/demo/saipe/2012-state-and-county.html \n",
    "\n",
    "(Note: It appears that the U.S. Census Department restructured its datafiles. When I originally downloaded the dataset, the 2011 and 2012 poverty estimates were combined into one file. Now, they exist in two separate files.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in third set of data\n",
    "df_poverty_org= pd.read_csv(r'C:\\Users\\t_gas\\Desktop\\Data Portfolio\\Data Storytelling_Exploring the Academic Achievement Gap\\2011_2012_School_Data\\Poverty_County\\Poverty_Estimates_County.csv')\n",
    "pd.set_option(\"display.max_columns\", 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After examining the first few rows of the dataset, I immediately notice that the county name is followed by the state abbreviation in parantheses. This abbreviation will have to be migrated to a new column, so that I can merge the poverty data with my other dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split column\n",
    "df_poverty_org['County'], df_poverty_org['State_2'] = df_poverty_org['State / County Name'].str.split('(', 1).str\n",
    "df_poverty_org['State_2'] = df_poverty_org['State_2'].str.replace(r\"\\.*\\)\",\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will create a smaller database that contains only the information about children ages 5-17, whose families are in poverty. This is the information provided by the Census Bureau to the U.S. Department of Education, who uses such information to determine the correct distribution of Title 1 funds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create smaller dataframe with only necessary columns\n",
    "df_poverty = df_poverty_org[['Year', 'State / County Name', 'County', 'State_2', 'Ages 5 to 17 in Families in Poverty Percent']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    ".info() provides more information about how the poverty information is stored. Thankfully, the percentages of children living in families below the poverty line have already been saved as float64 objects.  \n",
    "\n",
    "By running \"df_poverty.isnull().sum(), I can see how many NaN values are within each row of the dataframe. The NaN values correctly saved and the total (104 for the column \"State_2\") is fairly low and will not invalidate the data. \n",
    "\n",
    "The American Community Survey (where this poverty data is derived from) occurs once every calendar year. However, test scores are measured at the end of a school year (which lasts between two calendar years). Therefore, I will take the mean of the 2011 and 2012 poverty percentages for each county. To do this, I need to reshape the dataframe.\n",
    "\n",
    "Originally, I grouped the data based on the column name \"County.\" However, this resulted in a significant amount of lost data (because county names repeated within different states). To solve this problem, I grouped by the column \"State / County Name,\" which included both the state and county name in one column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new dataframe grouped by \"State/County Name\"\n",
    "df_poverty_county_means_org = df_poverty.groupby(['State / County Name'])['Ages 5 to 17 in Families in Poverty Percent'].mean()\n",
    "df_poverty_county_means = df_poverty_county_means_org.to_frame()\n",
    "df_poverty_county_means_indexed = df_poverty_county_means.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will now have to separate the county and state into two separate columns like I did before, creating a new dataframe using .groupby() and .mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split columns\n",
    "df_poverty_county_means_indexed['County'], df_poverty_county_means_indexed['State'] = df_poverty_county_means_indexed['State / County Name'].str.split('(', 1).str\n",
    "df_poverty_county_means_indexed['State'] = df_poverty_county_means_indexed['State'].str.replace(r\"\\.*\\)\",\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe above (\"df_poverty_county_means_indexed\") is now organized by county, while my state assessment dataframe (\"df_all_states\") is organized by school district. Unfortunately I cannot directly merge the two dataframes, because not every school district directly corresponds to a county. \n",
    "\n",
    "To correctly filter and subsequently merge the dataframes, I will use the same logic as the Census Bureau uses when determining Title 1 funding for school districts. The Bureau creates a subset of regular school districts from the larger NCES CCD Local Education Agency universe - \"regular\" being defined as school districts which are geographically defined. (See https://nces.ed.gov/programs/edge/geographicDistrictBoundary.aspx for more information.) \n",
    "\n",
    "__Data Source:__ https://nces.ed.gov/ccd/pubagency.asp (2011-2012)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in fourth set of data\n",
    "df_district_county_org = pd.read_excel(r'C:\\Users\\t_gas\\Desktop\\Data Portfolio\\Data Storytelling_Exploring the Academic Achievement Gap\\2011_2012_School_Data\\School District Info\\District_Data.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After importing the original dataframe, I create a copy with just the relevant columns. I then rename certain columns within the smaller dataframe, to better correspond with the information in my state assessment scores dataframes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a smaller dataframe with only necessary columns\n",
    "df_district_county = df_district_county_org[['LEAID', 'STID', 'NAME', 'LSTATE', 'TYPE','CONAME', 'BIEA']].copy()\n",
    "#rename columns\n",
    "df_district_county.rename(columns = {'NAME': 'leanm11', 'CONAME': 'County'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will now create a smaller database, where only rows with the column \"Type\" = 1 or the column \"BIEA\" = 2 are preserved. This is based on the Census Bureau's calculations, as mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter dataframe \n",
    "df_regular_district_county = df_district_county[df_district_county['TYPE'] == 1]\n",
    "df_regular_district_county_final = df_regular_district_county[df_regular_district_county['BIEA'] == 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before merging the state assessment dataframe with the regular districts dataframe, I will compare how many rows exist in each dataframe using .info(). In doing so, I see that there are approximately 3,000 more entries in the state assessment dataframe. This means that when I merge the two dataframes, I should use an outer join to ensure that these 3,000 rows are not dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dictionary of state abbreviations\n",
    "state_abbreviations = {\n",
    "    'Alabama': 'AL',\n",
    "    'Alaska': 'AK',\n",
    "    'Arizona': 'AZ',\n",
    "    'Arkansas': 'AR',\n",
    "    'California': 'CA',\n",
    "    'Colorado': 'CO',\n",
    "    'Connecticut': 'CT',\n",
    "    'Delaware': 'DE',\n",
    "    'Florida': 'FL',\n",
    "    'Georgia': 'GA',\n",
    "    'Hawaii': 'HI',\n",
    "    'Idaho': 'ID',\n",
    "    'Illinois': 'IL',\n",
    "    'Indiana': 'IN',\n",
    "    'Iowa': 'IA',\n",
    "    'Kansas': 'KS',\n",
    "    'Kentucky': 'KY',\n",
    "    'Louisiana': 'LA',\n",
    "    'Maine': 'ME',\n",
    "    'Maryland': 'MD',\n",
    "    'Massachusetts': 'MA',\n",
    "    'Michigan': 'MI',\n",
    "    'Minnesota': 'MN',\n",
    "    'Mississippi': 'MS',\n",
    "    'Missouri': 'MO',\n",
    "    'Montana': 'MT',\n",
    "    'Nebraska': 'NE',\n",
    "    'Nevada': 'NV',\n",
    "    'New Hampshire': 'NH',\n",
    "    'New Jersey': 'NJ',\n",
    "    'New Mexico': 'NM',\n",
    "    'New York': 'NY',\n",
    "    'North Carolina': 'NC',\n",
    "    'North Dakota': 'ND',\n",
    "    'Ohio': 'OH',\n",
    "    'Oklahoma': 'OK',\n",
    "    'Oregon': 'OR',\n",
    "    'Pennsylvania': 'PA',\n",
    "    'Rhode Island': 'RI',\n",
    "    'South Carolina': 'SC',\n",
    "    'South Dakota': 'SD',\n",
    "    'Tennessee': 'TN',\n",
    "    'Texas': 'TX',\n",
    "    'Utah': 'UT',\n",
    "    'Vermont': 'VT',\n",
    "    'Virginia': 'VA',\n",
    "    'Washington': 'WA',\n",
    "    'West Virginia': 'WV',\n",
    "    'Wisconsin': 'WI',\n",
    "    'Wyoming': 'WY',\n",
    "    'Bureau Of Indian Affairs' : 'BOIA'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge dataframes\n",
    "df_scores_districts_counties_org = pd.merge(df_all_states, df_regular_district_county_final, on = 'LEAID', how = 'outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    ".info() allows me to examine the new merged dataframe and check that no entries have been dropped.\n",
    "\n",
    "I am now almost ready to merge the two datasets. I will have to add one final column to \"df_scores_districts_counties,\" \n",
    "#which contains state abbreviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add column to dataframe\n",
    "df_scores_districts_counties_org['STNAM'] = df_scores_districts_counties_org['STNAM'].str.title() \n",
    "df_scores_districts_counties_org['State'] = df_scores_districts_counties_org['STNAM'].map(state_abbreviations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Now that the state abbreviations have been added, I will remove unnecessary columns and reorder the remaining columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove unnecessary columns and reorder remaining columns\n",
    "df_scores_districts_counties = df_scores_districts_counties_org[['County', 'State', 'LEAID', 'leanm11_x', 'leanm11_y', 'ALL_MTH00pctprof_1112', 'ALL_RLA00pctprof_1112']].copy()\n",
    "df_scores_districts_counties['County'] = df_scores_districts_counties['County'].str.title() \n",
    "df_scores_districts_counties['State/County'] = df_scores_districts_counties['County'] + ' ' + df_scores_districts_counties['State'].map(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use the same process as above with the poverty database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove unnecessary columns and reorder remaining columns\n",
    "df_poverty_county_means_indexed_small = df_poverty_county_means_indexed[['County', 'State', 'Ages 5 to 17 in Families in Poverty Percent']].copy()\n",
    "df_poverty_county_means_indexed_small['State/County'] = df_poverty_county_means_indexed_small['County'] + '' + df_poverty_county_means_indexed_small['State'].map(str)\n",
    "df_poverty_county_means_indexed_small = df_poverty_county_means_indexed_small.sort_values(by=['State'], ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dictionary of poverty levels per county\n",
    "df_poverty_dictionary = df_poverty_county_means_indexed_small[['Ages 5 to 17 in Families in Poverty Percent', 'State/County']].copy()\n",
    "poverty_dictionary = df_poverty_dictionary.set_index('State/County').T.to_dict('list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new column in dataframe\n",
    "df_scores_districts_counties['Ages 5 to 17 in Families in Poverty Percent'] = np.nan\n",
    "\n",
    "#add in values from dictionary\n",
    "for j in poverty_dictionary:\n",
    "    df_scores_districts_counties.loc[df_scores_districts_counties['State/County']==j,['Ages 5 to 17 in Families in Poverty Percent']] = poverty_dictionary[j] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after checking the amount of nan data using .info(), I create final dataframe \n",
    "df_final = df_scores_districts_counties[['LEAID', 'County', 'State', 'State/County', 'leanm11_x', 'ALL_MTH00pctprof_1112', 'ALL_RLA00pctprof_1112', 'Ages 5 to 17 in Families in Poverty Percent']].copy()\n",
    "df_final.rename(columns = {'leanm11_x':'School District' }, inplace=True) \n",
    "df_final = df_final.dropna(axis=0, how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export final dataset as .csv file\n",
    "\n",
    "df_final.to_csv(\"df_final.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
